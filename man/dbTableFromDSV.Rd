% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbTableFromDSV.R
\name{dbTableFromDSV}
\alias{dbTableFromDSV}
\title{Create a table from a delimiter separated values (DSV) text file}
\usage{
dbTableFromDSV(
  input_file,
  dbcon,
  table_name,
  header = TRUE,
  sep = ",",
  dec = ".",
  grp = "",
  id_quote_method = "DB_NAMES",
  col_names = NULL,
  col_types = NULL,
  col_import = NULL,
  drop_table = FALSE,
  auto_pk = FALSE,
  build_pk = FALSE,
  pk_fields = NULL,
  constant_values = NULL,
  chunk_size = 0,
  ...
)
}
\arguments{
\item{input_file}{character, the file name (including path) to be read.}

\item{dbcon}{database connection, as created by the dbConnect function.}

\item{table_name}{character, the name of the table.}

\item{header}{logical, if \code{TRUE} the first line contains the columns'
names. If \code{FALSE}, the columns' names will be formed sing a "V"
followed by the column number (as specified in \code{\link[utils:read.table]{utils::read.table()}}).}

\item{sep}{character, field delimiter (e.g., "," for CSV, "\\t" for TSV)
in the input file. Defaults to ",".}

\item{dec}{character, decimal separator (e.g., "." or "," depending on
locale) in the input file. Defaults to ".".}

\item{grp}{character, character used for digit grouping. It defaults
to \code{""} (i.e. no grouping).}

\item{id_quote_method}{character, used to specify how to build the SQLite
columns' names using the fields' identifiers read from the input file.
For details see the description of the \code{quote_method} parameter of
the \code{\link[=format_column_names]{format_column_names()}} function. Defautls to \code{DB_NAMES}.}

\item{col_names}{character vector, names of the columuns in the input file.
Used to override the field names derived from the input file (using the
quote method selected by \code{id_quote_method}). Must be of the same length
of the number of columns in the input file. If \code{NULL} the column names
coming from the input file (after quoting) will be used.
Defaults to \code{NULL}.}

\item{col_types}{character vector of classes to be assumed for the columns
of the input file. Must be of the same length of the number of columns
in the input file. If not null, it will override the data types guessed
from the input file.
If \code{NULL} the data type inferred from the input files will be used.
Defaults to \code{NULL}.}

\item{col_import}{can be either:
\itemize{
\item a numeric vector (coherced to integers) with the columns' positions
in the input file that will be imported in the SQLite table;
\item a character vector with the columns' names to be imported. The names
are those in the input file (after quoting with \code{id_quote_method}),
if \code{col_names} is NULL, or those expressed in \code{col_names} vector.
Defaults to NULL, i.e. all columns will be imported.
}}

\item{drop_table}{logical, if \code{TRUE} the target table will be dropped
(if exists) and recreated before importing the data.  if \code{FALSE}, data
from input file will be appended to an existing table.
Defaults to \code{FALSE}.}

\item{auto_pk}{logical, if \code{TRUE}, and \code{pk_fields} parameter is \code{NULL}, an
additional column named \code{SEQ} will be added to the table and it will be
defined to be \verb{INTEGER PRIMARY KEY} (i.e. in effect an alias for
\code{ROWID}). Defaults to \code{FALSE}.}

\item{build_pk}{logical, if \code{TRUE} creates a \verb{UNIQUE INDEX} named
\verb{<table_name>_PK} defined by the combination of fields specified
in the \code{pk_fields} parameter. It will be effective only if
\code{pk_fields} is not null. Defaults to \code{FALSE}.}

\item{pk_fields}{character vector, the list of the fields' names that
define the \verb{UNIQUE INDEX}. Defaults to \code{NULL}.}

\item{constant_values}{a one row data frame whose columns will be added to
the table in the database. The additional table columns will be named
as the data frame columns, and the corresponding values will be associeted
to each record imported from the input file. It is useful to keep
track of additional information (e.g., the input file name, additional
context data not available in the data set, ...) when loading
the content of multiple input files in the same table.}

\item{chunk_size}{integer, the number of lines in each "chunk" (i.e. block
of lines from the input file). Setting its value to a positive integer
number, will process the input file by blocks of \code{chunk_size} lines,
avoiding to read all the data in memory at once. It can be useful
for very large size files. If set to zero, it will process the whole
text file in one pass. Default to zero.}

\item{...}{additional arguments passed to \code{\link[base:scan]{base::scan()}} function used
to read input data. Please note that if the \code{quote} parameter is not
specified, it will be set to \code{""} (i.e., no quoting) by default.}
}
\value{
integer, the number of records in \code{table_name} after reading data
from \code{input_file}.
}
\description{
The \code{dbTableFromDSV()} function reads the data from a DSV file
and copies it to a table in a SQLite database. If table does
not exist, it will create it.

The \code{dbTableFromDSV()} function reads the data from a DSV file
and copies it to a table in a SQLite database. If table does
not exist, it will create it.
}
